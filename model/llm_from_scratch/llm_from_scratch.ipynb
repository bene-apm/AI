{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a34efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7b732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "block_size = 32 \n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c9d4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11bc94b70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b0b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ประมวลกฎหมายอาญา.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7558b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ()-./:[]กขคฆงจฉชซญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรฤลวศษสหอฯะัาำิีึืุูเแโใไๆ็่้๋์๐๑๒๓๔๕๖๗๘๙“”\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa55bdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '(': 2, ')': 3, '-': 4, '.': 5, '/': 6, ':': 7, '[': 8, ']': 9, 'ก': 10, 'ข': 11, 'ค': 12, 'ฆ': 13, 'ง': 14, 'จ': 15, 'ฉ': 16, 'ช': 17, 'ซ': 18, 'ญ': 19, 'ฎ': 20, 'ฏ': 21, 'ฐ': 22, 'ฑ': 23, 'ฒ': 24, 'ณ': 25, 'ด': 26, 'ต': 27, 'ถ': 28, 'ท': 29, 'ธ': 30, 'น': 31, 'บ': 32, 'ป': 33, 'ผ': 34, 'ฝ': 35, 'พ': 36, 'ฟ': 37, 'ภ': 38, 'ม': 39, 'ย': 40, 'ร': 41, 'ฤ': 42, 'ล': 43, 'ว': 44, 'ศ': 45, 'ษ': 46, 'ส': 47, 'ห': 48, 'อ': 49, 'ฯ': 50, 'ะ': 51, 'ั': 52, 'า': 53, 'ำ': 54, 'ิ': 55, 'ี': 56, 'ึ': 57, 'ื': 58, 'ุ': 59, 'ู': 60, 'เ': 61, 'แ': 62, 'โ': 63, 'ใ': 64, 'ไ': 65, 'ๆ': 66, '็': 67, '่': 68, '้': 69, '๋': 70, '์': 71, '๐': 72, '๑': 73, '๒': 74, '๓': 75, '๔': 76, '๕': 77, '๖': 78, '๗': 79, '๘': 80, '๙': 81, '“': 82, '”': 83}\n",
      "---\n",
      "{0: '\\n', 1: ' ', 2: '(', 3: ')', 4: '-', 5: '.', 6: '/', 7: ':', 8: '[', 9: ']', 10: 'ก', 11: 'ข', 12: 'ค', 13: 'ฆ', 14: 'ง', 15: 'จ', 16: 'ฉ', 17: 'ช', 18: 'ซ', 19: 'ญ', 20: 'ฎ', 21: 'ฏ', 22: 'ฐ', 23: 'ฑ', 24: 'ฒ', 25: 'ณ', 26: 'ด', 27: 'ต', 28: 'ถ', 29: 'ท', 30: 'ธ', 31: 'น', 32: 'บ', 33: 'ป', 34: 'ผ', 35: 'ฝ', 36: 'พ', 37: 'ฟ', 38: 'ภ', 39: 'ม', 40: 'ย', 41: 'ร', 42: 'ฤ', 43: 'ล', 44: 'ว', 45: 'ศ', 46: 'ษ', 47: 'ส', 48: 'ห', 49: 'อ', 50: 'ฯ', 51: 'ะ', 52: 'ั', 53: 'า', 54: 'ำ', 55: 'ิ', 56: 'ี', 57: 'ึ', 58: 'ื', 59: 'ุ', 60: 'ู', 61: 'เ', 62: 'แ', 63: 'โ', 64: 'ใ', 65: 'ไ', 66: 'ๆ', 67: '็', 68: '่', 69: '้', 70: '๋', 71: '์', 72: '๐', 73: '๑', 74: '๒', 75: '๓', 76: '๔', 77: '๕', 78: '๖', 79: '๗', 80: '๘', 81: '๙', 82: '“', 83: '”'}\n",
      "---\n",
      "[10, 20, 48, 39, 53, 40]\n",
      "['ก', 'ฎ', 'ห', 'ม', 'า', 'ย']\n",
      "กฎหมาย\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "pre_decode = lambda l: [itos[i] for i in l]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(stoi)\n",
    "print('---')\n",
    "print(itos)\n",
    "print('---')\n",
    "print(encode('กฎหมาย'))\n",
    "print(pre_decode(encode('กฎหมาย')))\n",
    "print(decode(encode('กฎหมาย')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d91553",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7019f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21218 M parameters\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x) \n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) \n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "    \n",
    "        tok_emb = self.token_embedding_table(idx) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = tok_emb + pos_emb \n",
    "        x = self.blocks(x) \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "          \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "   \n",
    "            logits, loss = self(idx_cond)\n",
    "           \n",
    "            logits = logits[:, -1, :] \n",
    "         \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "           \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "         \n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1794a479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5728, val loss 4.5425\n",
      "step 100: train loss 2.8638, val loss 3.0167\n",
      "step 200: train loss 2.5459, val loss 2.5134\n",
      "step 300: train loss 2.2607, val loss 2.2267\n",
      "step 400: train loss 2.0289, val loss 1.9973\n",
      "step 500: train loss 1.8656, val loss 1.8677\n",
      "step 600: train loss 1.7330, val loss 1.7938\n",
      "step 700: train loss 1.5998, val loss 1.7048\n",
      "step 800: train loss 1.5563, val loss 1.6997\n",
      "step 900: train loss 1.4753, val loss 1.6726\n",
      "step 1000: train loss 1.4475, val loss 1.6490\n",
      "step 1100: train loss 1.3849, val loss 1.6122\n",
      "step 1200: train loss 1.3339, val loss 1.6134\n",
      "step 1300: train loss 1.3252, val loss 1.6266\n",
      "step 1400: train loss 1.2723, val loss 1.6031\n",
      "step 1500: train loss 1.2307, val loss 1.6189\n",
      "step 1600: train loss 1.2287, val loss 1.6066\n",
      "step 1700: train loss 1.2133, val loss 1.5759\n",
      "step 1800: train loss 1.1878, val loss 1.5558\n",
      "step 1900: train loss 1.1757, val loss 1.5331\n",
      "step 2000: train loss 1.1712, val loss 1.5678\n",
      "step 2100: train loss 1.1241, val loss 1.5457\n",
      "step 2200: train loss 1.1426, val loss 1.5288\n",
      "step 2300: train loss 1.1102, val loss 1.5169\n",
      "step 2400: train loss 1.0896, val loss 1.5192\n",
      "step 2500: train loss 1.1214, val loss 1.5480\n",
      "step 2600: train loss 1.0696, val loss 1.5043\n",
      "step 2700: train loss 1.0833, val loss 1.4999\n",
      "step 2800: train loss 1.0619, val loss 1.5253\n",
      "step 2900: train loss 1.0535, val loss 1.4864\n",
      "step 3000: train loss 1.0513, val loss 1.5188\n",
      "step 3100: train loss 1.0425, val loss 1.4685\n",
      "step 3200: train loss 0.9999, val loss 1.4820\n",
      "step 3300: train loss 1.0028, val loss 1.4651\n",
      "step 3400: train loss 0.9976, val loss 1.4804\n",
      "step 3500: train loss 1.0143, val loss 1.4556\n",
      "step 3600: train loss 1.0041, val loss 1.4829\n",
      "step 3700: train loss 0.9910, val loss 1.4641\n",
      "step 3800: train loss 0.9936, val loss 1.4712\n",
      "step 3900: train loss 0.9803, val loss 1.4108\n",
      "step 4000: train loss 0.9693, val loss 1.4231\n",
      "step 4100: train loss 0.9611, val loss 1.4434\n",
      "step 4200: train loss 0.9718, val loss 1.3901\n",
      "step 4300: train loss 0.9550, val loss 1.4113\n",
      "step 4400: train loss 0.9393, val loss 1.4376\n",
      "step 4500: train loss 0.9532, val loss 1.4568\n",
      "step 4600: train loss 0.9422, val loss 1.4571\n",
      "step 4700: train loss 0.9291, val loss 1.4674\n",
      "step 4800: train loss 0.9397, val loss 1.4129\n",
      "step 4900: train loss 0.9267, val loss 1.4126\n",
      "step 4999: train loss 0.9181, val loss 1.4389\n",
      "\n",
      "เมาตรา ๒๑   ผู้ใดโัตจำนวนโทษสำหรับความผิดนี้อยเหตุลาใช้บุคคลจะเด็กหรือแสวันไม่ลงโทษจำคุกไม่เกินเจ็ดปี หรือปรับตั้งแต่สามนั้นขึ้นเป็นอัยตัวในเจ้า” หมายเหลของ ต้องระวางโทษกษจำคุกตั้งแต่วันถัดจากวันประโยกันหรือจะปรับไว้เพ.ศ. ๑๔๕๙] ให้นำนวณระหวังความผิดเป็นของตนเองตนา พระรับตการหรือได้ก่อเจ้าพนการเครื่องจะต้องรับผิด หามรับเสีย หรือซ่อมาใช้ให้ละเป็นอันฐานเป็นส่วนใดที่พ้นโทษจากเป็นโทษตามมาตรา ๓๐๖ แห่งพระราชบัญญัติแห่งกฎหมายอื่นลับโทษไม่เป็นความผิดฐานในที่บัญญัติไว้ในมาตรา ๑๙๘  ถ้างถึงสำหรับมาใช้บังคับ ซึ่งรัฐบาลวะอการของผู้อื่นหรือประชาชน ต้องระวางโทษจากผู้ถูกฟ้องขู่แองตนด้วยความอันที่ใด ๆ หรือกระทำความ หรือปรับตั้งแต่วันถามผิดนั้น  ถ้าผู้อยู่ในฟ้างในลักษณะผู้ได้รับสวนที่ปกอั่งซึ่งเด็กนั้นต่ำความผิดด้วยหนังอยู่ก็อกสิกร้ได้มีการยุติธรรมบัญญัติแหน่วนเห้า การประกอื่นเกิด หรือการกระทำแสดงอาณาจักรรภาพหรือเสรีภาพการกำหนดังต่อไปนี้ให้บุคคลที่ได้มีกับตามบทบังและการที่ผู้กระทำความผิดอารในคตออา ต้องระวางโทษผู้นั้นกระทำความผิดฐานกระชนวัดและเสรียง หาแค่าทำให้สูงพัตว์ขัดขืนเป็นการไปอ่าว่า เหยียดหยาม จนเมื่อกำหนดกาลงหรือสวกสาธารณประโยชนเปราะการโทรอกถีก ต้องระวางโทษจำคุกตัวอัยเหตุสมาตรา (ฉบับที่ ๒๖) พ.ศ. ๒๕๖๐]\n",
      " \n",
      "มาตรา ๒  ผู้ใดหลบริบวงับในการชั่งประเทศ หรือทวยาสาธรรณประเบิด ๆ หรือผู้อยึดส่วน หรือประเทศ ให้ประชาช่วยหรือผู้โยชั่วแสำนะสิบปี และปรับตั้งแต่วัดหน่วยหน่ด้วย ได้เก็ดียหรือบวกันหรือสาธารณของการแทนพิจารณาของทางอาญา เช่นนั้นเดินทางปรากระทั้งน่าจะเมียหน้าพันธประกันไม่มีกฎหมายนั้นอพ้างความผิดฐานตามวรรคหนึ่งหนึ่ง[๑]\n",
      "ผู้กระทำอันเช่นแท่ว่าเพื่อให้ผู้อื่นได้หรือผู้อื่นได้กระทำใดด้วยที่ใช้ดูหมิ่นิบรัชทานหรือสนานไว้อื่น ศาลเห็นสมอาชัวปรับและความผิดฐานหมิ่นปกระทำการใดใช้กำตรอันเปลาภอก ต้องระวางโทษดังที่บัญญัติไว้ในความผิดเหรือเช่นว่านั้นที่ปรากฏวิธารณดังต่อหรือสั่งแถึงผู้ที่อยู่สาธารณสถานอันยึงบัดความผิดดังนั้นเป็นคุณ รักยอม หรือการกระทำนั้นได้เกิดอันตรายแก่ประชาชนแผ่นน่วตามมาตรา ๒๗๓ ถึงมาทธิดียอน สาธารเฉพิ่วันภัน และไม่แหละความมุ่นสาม หรือง และบุคคลใดผลห่มีผู้อื่นกระทำความผิดเสียจากเข้ากระทำความผิด และโอกลักษณะอาญา และถัดจากวันประกาศในราชการแทนะยะการที่น่าจะกับเจ็ดเผ้อมีความแต่สมควรและออกใช\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "  \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
