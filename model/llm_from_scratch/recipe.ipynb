{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93197136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bb26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "block_size = 32 \n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca475f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d290bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcae87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('thai green curry.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555f8dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ()-.123456789กขคงจฉชซณดตถทธนบปผพฟภมยรลวษสหอะัาำิีึืุูเแโใไๆ็่้์\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b6f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '(': 2, ')': 3, '-': 4, '.': 5, '1': 6, '2': 7, '3': 8, '4': 9, '5': 10, '6': 11, '7': 12, '8': 13, '9': 14, 'ก': 15, 'ข': 16, 'ค': 17, 'ง': 18, 'จ': 19, 'ฉ': 20, 'ช': 21, 'ซ': 22, 'ณ': 23, 'ด': 24, 'ต': 25, 'ถ': 26, 'ท': 27, 'ธ': 28, 'น': 29, 'บ': 30, 'ป': 31, 'ผ': 32, 'พ': 33, 'ฟ': 34, 'ภ': 35, 'ม': 36, 'ย': 37, 'ร': 38, 'ล': 39, 'ว': 40, 'ษ': 41, 'ส': 42, 'ห': 43, 'อ': 44, 'ะ': 45, 'ั': 46, 'า': 47, 'ำ': 48, 'ิ': 49, 'ี': 50, 'ึ': 51, 'ื': 52, 'ุ': 53, 'ู': 54, 'เ': 55, 'แ': 56, 'โ': 57, 'ใ': 58, 'ไ': 59, 'ๆ': 60, '็': 61, '่': 62, '้': 63, '์': 64}\n",
      "---\n",
      "{0: '\\n', 1: ' ', 2: '(', 3: ')', 4: '-', 5: '.', 6: '1', 7: '2', 8: '3', 9: '4', 10: '5', 11: '6', 12: '7', 13: '8', 14: '9', 15: 'ก', 16: 'ข', 17: 'ค', 18: 'ง', 19: 'จ', 20: 'ฉ', 21: 'ช', 22: 'ซ', 23: 'ณ', 24: 'ด', 25: 'ต', 26: 'ถ', 27: 'ท', 28: 'ธ', 29: 'น', 30: 'บ', 31: 'ป', 32: 'ผ', 33: 'พ', 34: 'ฟ', 35: 'ภ', 36: 'ม', 37: 'ย', 38: 'ร', 39: 'ล', 40: 'ว', 41: 'ษ', 42: 'ส', 43: 'ห', 44: 'อ', 45: 'ะ', 46: 'ั', 47: 'า', 48: 'ำ', 49: 'ิ', 50: 'ี', 51: 'ึ', 52: 'ื', 53: 'ุ', 54: 'ู', 55: 'เ', 56: 'แ', 57: 'โ', 58: 'ใ', 59: 'ไ', 60: 'ๆ', 61: '็', 62: '่', 63: '้', 64: '์'}\n",
      "---\n",
      "[56, 15, 18]\n",
      "['แ', 'ก', 'ง']\n",
      "แกง\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "pre_decode = lambda l: [itos[i] for i in l]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(stoi)\n",
    "print('---')\n",
    "print(itos)\n",
    "print('---')\n",
    "print(encode('แกง'))\n",
    "print(pre_decode(encode('แกง')))\n",
    "print(decode(encode('แกง')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2668c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2833e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x) \n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) \n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "    \n",
    "        tok_emb = self.token_embedding_table(idx) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = tok_emb + pos_emb \n",
    "        x = self.blocks(x) \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "          \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "   \n",
    "            logits, loss = self(idx_cond)\n",
    "           \n",
    "            logits = logits[:, -1, :] \n",
    "         \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "           \n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "         \n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a58689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3463, val loss 4.3735\n",
      "step 100: train loss 2.4102, val loss 3.1430\n",
      "step 200: train loss 1.7384, val loss 3.0545\n",
      "step 300: train loss 1.0356, val loss 3.1172\n",
      "step 400: train loss 0.5564, val loss 3.4640\n",
      "step 500: train loss 0.3422, val loss 3.6903\n",
      "step 600: train loss 0.2747, val loss 3.9265\n",
      "step 700: train loss 0.2379, val loss 4.1296\n",
      "step 800: train loss 0.2095, val loss 4.2324\n",
      "step 900: train loss 0.2006, val loss 4.3205\n",
      "step 1000: train loss 0.1913, val loss 4.4204\n",
      "step 1100: train loss 0.1806, val loss 4.5349\n",
      "step 1200: train loss 0.1709, val loss 4.6064\n",
      "step 1300: train loss 0.1737, val loss 4.6378\n",
      "step 1400: train loss 0.1714, val loss 4.7553\n",
      "step 1500: train loss 0.1665, val loss 4.6400\n",
      "step 1600: train loss 0.1668, val loss 4.7125\n",
      "step 1700: train loss 0.1672, val loss 4.7119\n",
      "step 1800: train loss 0.1620, val loss 4.7055\n",
      "step 1900: train loss 0.1530, val loss 4.8309\n",
      "step 2000: train loss 0.1582, val loss 4.7945\n",
      "step 2100: train loss 0.1552, val loss 4.8778\n",
      "step 2200: train loss 0.1555, val loss 4.8466\n",
      "step 2300: train loss 0.1549, val loss 4.7508\n",
      "step 2400: train loss 0.1533, val loss 4.8386\n",
      "step 2500: train loss 0.1503, val loss 4.9232\n",
      "step 2600: train loss 0.1519, val loss 5.0158\n",
      "step 2700: train loss 0.1537, val loss 5.0457\n",
      "step 2800: train loss 0.1540, val loss 4.9620\n",
      "step 2900: train loss 0.1479, val loss 5.0453\n",
      "step 3000: train loss 0.1513, val loss 5.0967\n",
      "step 3100: train loss 0.1497, val loss 4.9929\n",
      "step 3200: train loss 0.1493, val loss 4.9289\n",
      "step 3300: train loss 0.1520, val loss 5.1122\n",
      "step 3400: train loss 0.1441, val loss 4.9716\n",
      "step 3500: train loss 0.1475, val loss 5.2329\n",
      "step 3600: train loss 0.1456, val loss 5.2558\n",
      "step 3700: train loss 0.1435, val loss 5.0132\n",
      "step 3800: train loss 0.1451, val loss 5.1488\n",
      "step 3900: train loss 0.1433, val loss 5.0161\n",
      "step 4000: train loss 0.1427, val loss 5.0370\n",
      "step 4100: train loss 0.1497, val loss 4.9732\n",
      "step 4200: train loss 0.1444, val loss 4.9463\n",
      "step 4300: train loss 0.1460, val loss 5.1949\n",
      "step 4400: train loss 0.1455, val loss 5.1167\n",
      "step 4500: train loss 0.1401, val loss 5.1602\n",
      "step 4600: train loss 0.1399, val loss 5.1636\n",
      "step 4700: train loss 0.1426, val loss 5.0550\n",
      "step 4800: train loss 0.1457, val loss 4.9155\n",
      "step 4900: train loss 0.1433, val loss 5.1139\n",
      "step 4999: train loss 0.1396, val loss 5.0778\n",
      "\n",
      "2. หลังจากนั้นใส่เครื่องแกงเผ็ดประมาณ 1 ช้อนชา เพื่อเพิ่มความเผ็ด (ใส่ในปริมาณที่ต้องการได้)\n",
      "3. และสุดท้ายใส่กะปิลงไป ผสมให้เข้ากันเป็นอันเสร็จเรียบร้อย ก็จะได้เครื่องแกงเขียวหวานสำหรับนำไปประกอบอาหารในข้นมานิดหน่อยด้วยการเติมกะปิและเครื่องแกงเผ็ด ใส่เนื้อสัตว์ที่ต้องการ พร้อมปรุงรสตามความชอบ\n",
      "\n",
      "โดยขั้นตอนแรกจะต้องเริ่มจากการทำเครื่องแกงเขียวหวานก่อน แต่หากอยากข้ามขั้นตอนส่วนนี้ ก็สามารถหาซื้อเครื่องแกงเขียวหวานสำเร็จรูปมาใช้ได้เลย ทั้งนี้สามารถปรับปริมาณของส่วนผสมได้ตามที่ชอบ\n",
      "\n",
      "วัตถุดิบสำหรับทำเครื่องแกงเขียวหวาน \n",
      "- พริกขี้หนูเขียว \n",
      "- พริกไทยเม็ด \n",
      "- ลูกผักชี \n",
      "- เมล็ดยี่หร่า\n",
      "- เกลือ \n",
      "- หัวข่าซอยหยาบ \n",
      "- ตะไคร้ซอย \n",
      "- ผิวมะกรูด  \n",
      "- เครื่องแกงเผ็ด\n",
      "- กะปิ\n",
      "\n",
      "วิธีทำเครื่องแกงเขียวหวาน\n",
      "1. นำลูกผักชีกับเมล็ดยี่หร่าไปคั่วให้หอม แล้วนำมาตำรวมกับวัตถุดิบอื่นๆ จนละเอียด\n",
      "2. หลังจากนั้นใส่เครื่องแกงเผ็ดประมาณ 1 ช้อนชา เพื่อเพิ่มความเผ็ดร้อนผสมได้ตามที่ชอบ\n",
      "\n",
      "วัตถุดิบสำหรับทำเครื่องแกงเขียวหวาน \n",
      "- พริกขี้หนูเขียว \n",
      "- พริกไทยเม็ด \n",
      "- ลูกผักชี \n",
      "- เมล็ดยี่หร่า\n",
      "- เกลือ \n",
      "- หัวข่าซอยหยาบ \n",
      "- ตะไคร้ซอย \n",
      "- ผิวมะกรูด  \n",
      "- เครื่องแกงเผ็ด\n",
      "- กะปิ\n",
      "\n",
      "วิธีทำเครื่องแกงเขียวหวาน\n",
      "1. นำลูกผักชีกับเมล็ดยี่หร่าไปคั่วให้หอม แล้วนำมาตำรวมกับวัตถุดิบอื่นๆ จนละเอียด\n",
      "2. หลังจากนั้นใส่เครื่องแกงเผ็ดประมาณ 1 ช้อนช้อนชา เพื่อเพิ่มความเผ็ดร้อนขึ้นมานิดหน่อยด้วยการเติมกะปิและเครื่องแกงเผ็ด ใส่เนื้อสัตว์ที่ต้องการ พร้อมปรุงรสตามความชอบ\n",
      "\n",
      "โดยขั้นตอนแรกจะต้องเริ่มจากการทำเครื่องแกงเขียวหวานก่อน แต่หากอยากข้ามขั้นตอนส่วนนี้ ก็สามารถหาซื้อเครื่องแกงเขียวหวานสำเร็จรูปมาใช้ได้เลย ทั้งนี้สามารถปรับปริมาณของส่วนผสมได้ตามที่ชอบ\n",
      "\n",
      "วัตถุดิบสำหรับทำเครื่องแกงเขียวหวาน \n",
      "- พริกขี้หนูเขียว \n",
      "- พริกไทยเม็ด \n",
      "- ลูกผักชี \n",
      "- เมล็ดยี่หร่า\n",
      "- เกลือ \n",
      "- หัวข่าซอยหยาบ \n",
      "- ตะไคร้ซอย \n",
      "- ผิวมะกรูด  \n",
      "- เครื่องแกงเผ็ด\n",
      "- กะปิ\n",
      "\n",
      "วิธีทำเครื่องแกงเขียวหวาน\n",
      "1. นำลูกผักชีกับเมล็ดยี่หร่าไปคั่วให้หอม แล้วนำมาตำรวมกับวัตถุดิบอื่นๆ จนละเอียด\n",
      "2. หลังจากนั้นใส่เครื่องแกงเผ็ดประมาณ 1 ช้อนชา เพื่อเพิ่มความเผ็ด (ใส่ในปริมาณที่ต้องการได้)\n",
      "3. และสุดท้ายใส่กะปิลงไป ผสมให้เข้ากันเป็นอันเสร็จเรียบร้อย ก็จะได้เครื่องแกงเขียวหวานสำหรับนำไปประกอบอาหารในขั้นตอนต่อไป\n",
      "\n",
      "วัตถุดิบสำหรับท\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "  \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77216796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
